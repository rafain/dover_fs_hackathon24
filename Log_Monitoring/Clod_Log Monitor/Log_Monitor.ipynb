{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "732a9eab-be27-43e2-b76a-3db9dff8f505",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">import com.azure.cosmos.spark._\n",
       "import com.azure.cosmos.spark.CosmosConfig\n",
       "import com.azure.cosmos.spark.CosmosAccountConfig\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">import com.azure.cosmos.spark._\nimport com.azure.cosmos.spark.CosmosConfig\nimport com.azure.cosmos.spark.CosmosAccountConfig\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "import com.azure.cosmos.spark._\n",
    "import com.azure.cosmos.spark.CosmosConfig\n",
    "import com.azure.cosmos.spark.CosmosAccountConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecfa6e99-9c1f-42de-ba59-d2dc5a0e7c96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">strCosmosDBUriConfigValue: String = https://dfsnosql.documents.azure.com:443/\n",
       "strCosmosDBKeyConfigValue: String = 81tg5e5reoGIqfNQvi108FUtW32axXwWXP4t9xzTeVCOlf47zoL2aX2i75FWf2AwBl5VaTcr9dWeACDb30nO0w==\n",
       "strCosmosDBDatabaseName: String = dfslogs\n",
       "strCosmosDBCollectionName: String = logs\n",
       "volcanoDBConfig: scala.collection.immutable.Map[String,String] = Map(spark.cosmos.read.customQuery -&gt; SELECT * FROM c, spark.cosmos.accountKey -&gt; 81tg5e5reoGIqfNQvi108FUtW32axXwWXP4t9xzTeVCOlf47zoL2aX2i75FWf2AwBl5VaTcr9dWeACDb30nO0w==, spark.cosmos.database -&gt; dfslogs, spark.cosmos.container -&gt; logs, spark.cosmos.accountEndpoint -&gt; https://dfsnosql.documents.azure.com:443/)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">strCosmosDBUriConfigValue: String = https://dfsnosql.documents.azure.com:443/\nstrCosmosDBKeyConfigValue: String = 81tg5e5reoGIqfNQvi108FUtW32axXwWXP4t9xzTeVCOlf47zoL2aX2i75FWf2AwBl5VaTcr9dWeACDb30nO0w==\nstrCosmosDBDatabaseName: String = dfslogs\nstrCosmosDBCollectionName: String = logs\nvolcanoDBConfig: scala.collection.immutable.Map[String,String] = Map(spark.cosmos.read.customQuery -&gt; SELECT * FROM c, spark.cosmos.accountKey -&gt; 81tg5e5reoGIqfNQvi108FUtW32axXwWXP4t9xzTeVCOlf47zoL2aX2i75FWf2AwBl5VaTcr9dWeACDb30nO0w==, spark.cosmos.database -&gt; dfslogs, spark.cosmos.container -&gt; logs, spark.cosmos.accountEndpoint -&gt; https://dfsnosql.documents.azure.com:443/)\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "var strCosmosDBUriConfigValue = \"https://dfsnosql.documents.azure.com:443/\"\n",
    "var strCosmosDBKeyConfigValue = \"81tg5e5reoGIqfNQvi108FUtW32axXwWXP4t9xzTeVCOlf47zoL2aX2i75FWf2AwBl5VaTcr9dWeACDb30nO0w==\"\n",
    "var strCosmosDBDatabaseName = \"dfslogs\"\n",
    "var strCosmosDBCollectionName = \"logs\"\n",
    " \n",
    "val volcanoDBConfig = Map(\"spark.cosmos.accountEndpoint\" -> strCosmosDBUriConfigValue,\n",
    "       \"spark.cosmos.accountKey\" -> strCosmosDBKeyConfigValue,\n",
    "        \"spark.cosmos.database\" -> strCosmosDBDatabaseName,\n",
    "//         \"spark.cosmos.preferredRegions\" -> strCosmosDBRegionsConfigValue ,\n",
    "        \"spark.cosmos.container\" -> strCosmosDBCollectionName,\n",
    "       \"spark.cosmos.read.customQuery\" -> \"SELECT * FROM c\"\n",
    "   ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b707435e-6157-48bd-ac3f-a8fda2aaee45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\tat com.azure.cosmos.spark.RowSerializerPool$.getOrCreateSerializer(RowSerializerPool.scala:42)\n",
       "\tat com.azure.cosmos.spark.ItemsPartitionReader.&lt;init&gt;(ItemsPartitionReader.scala:200)\n",
       "\tat com.azure.cosmos.spark.ItemsScanPartitionReaderFactory.createReader(ItemsScanPartitionReaderFactory.scala:38)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.advanceToNextIter(DataSourceRDD.scala:84)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:88)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n",
       "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:196)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:181)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:146)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:146)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:929)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:102)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:932)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:824)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3586)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3518)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3507)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3507)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1515)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1515)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1515)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3832)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3744)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3732)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1239)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1227)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2961)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:355)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:299)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$collect$1(Collector.scala:383)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:380)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:122)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:131)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:93)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:89)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:77)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:505)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:499)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:516)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:393)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:393)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:315)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:520)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:517)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3672)\n",
       "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4595)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3380)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4586)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:998)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4584)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:295)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:554)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:216)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1127)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:153)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:503)\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4584)\n",
       "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3380)\n",
       "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3603)\n",
       "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:322)\n",
       "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:357)\n",
       "\tat org.apache.spark.sql.Dataset.show(Dataset.scala:903)\n",
       "\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3170521713410109:3)\n",
       "\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3170521713410109:56)\n",
       "\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3170521713410109:58)\n",
       "\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3170521713410109:60)\n",
       "\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3170521713410109:62)\n",
       "\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw$$iw$$iw.&lt;init&gt;(command-3170521713410109:64)\n",
       "\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw$$iw.&lt;init&gt;(command-3170521713410109:66)\n",
       "\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw.&lt;init&gt;(command-3170521713410109:68)\n",
       "\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read.&lt;init&gt;(command-3170521713410109:70)\n",
       "\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$.&lt;init&gt;(command-3170521713410109:74)\n",
       "\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$.&lt;clinit&gt;(command-3170521713410109)\n",
       "\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n",
       "\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$eval$.$print(&lt;notebook&gt;:6)\n",
       "\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$eval.$print(&lt;notebook&gt;)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n",
       "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n",
       "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n",
       "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n",
       "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n",
       "\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:236)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1427)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1380)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:236)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$34(DriverLocal.scala:1021)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$22(DriverLocal.scala:1004)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:204)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:946)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:866)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:858)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:654)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:812)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:531)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:447)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:290)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply(Lorg/apache/spark/sql/types/StructType;)Lorg/apache/spark/sql/catalyst/encoders/ExpressionEncoder;\n",
       "\tat com.azure.cosmos.spark.RowSerializerPool$.getOrCreateSerializer(RowSerializerPool.scala:42)\n",
       "\tat com.azure.cosmos.spark.ItemsPartitionReader.&lt;init&gt;(ItemsPartitionReader.scala:200)\n",
       "\tat com.azure.cosmos.spark.ItemsScanPartitionReaderFactory.createReader(ItemsScanPartitionReaderFactory.scala:38)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.advanceToNextIter(DataSourceRDD.scala:84)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:88)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n",
       "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:196)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:181)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:146)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:146)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:929)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:102)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:932)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:824)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\">\tat com.azure.cosmos.spark.RowSerializerPool$.getOrCreateSerializer(RowSerializerPool.scala:42)\n\tat com.azure.cosmos.spark.ItemsPartitionReader.&lt;init&gt;(ItemsPartitionReader.scala:200)\n\tat com.azure.cosmos.spark.ItemsScanPartitionReaderFactory.createReader(ItemsScanPartitionReaderFactory.scala:38)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.advanceToNextIter(DataSourceRDD.scala:84)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:196)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:181)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:146)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:146)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:929)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:102)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:932)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:824)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3586)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3518)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3507)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3507)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1515)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1515)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1515)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3832)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3744)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3732)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1239)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1227)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2961)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:355)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:299)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$collect$1(Collector.scala:383)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:380)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:122)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:131)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:93)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:89)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:77)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:505)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:499)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:516)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:393)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:393)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:315)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:520)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:517)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3672)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4595)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3380)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4586)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:998)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4584)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:295)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:554)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:216)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:153)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:503)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4584)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3380)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3603)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:322)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:357)\n\tat org.apache.spark.sql.Dataset.show(Dataset.scala:903)\n\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3170521713410109:3)\n\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3170521713410109:56)\n\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3170521713410109:58)\n\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3170521713410109:60)\n\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3170521713410109:62)\n\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw$$iw$$iw.&lt;init&gt;(command-3170521713410109:64)\n\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw$$iw.&lt;init&gt;(command-3170521713410109:66)\n\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$$iw.&lt;init&gt;(command-3170521713410109:68)\n\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read.&lt;init&gt;(command-3170521713410109:70)\n\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$.&lt;init&gt;(command-3170521713410109:74)\n\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$read$.&lt;clinit&gt;(command-3170521713410109)\n\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$eval$.$print(&lt;notebook&gt;:6)\n\tat $linec3baa65d9bc94da5a1e9a1b6cefe33f729.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:236)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1427)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1380)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:236)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$34(DriverLocal.scala:1021)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$22(DriverLocal.scala:1004)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:204)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:946)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:866)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:858)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:654)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:812)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:531)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:447)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:290)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply(Lorg/apache/spark/sql/types/StructType;)Lorg/apache/spark/sql/catalyst/encoders/ExpressionEncoder;\n\tat com.azure.cosmos.spark.RowSerializerPool$.getOrCreateSerializer(RowSerializerPool.scala:42)\n\tat com.azure.cosmos.spark.ItemsPartitionReader.&lt;init&gt;(ItemsPartitionReader.scala:200)\n\tat com.azure.cosmos.spark.ItemsScanPartitionReaderFactory.createReader(ItemsScanPartitionReaderFactory.scala:38)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.advanceToNextIter(DataSourceRDD.scala:84)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:196)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:181)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:146)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:146)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:929)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:102)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:932)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:824)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)</div>",
       "errorSummary": "org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply(Lorg/apache/spark/sql/types/StructType;)Lorg/apache/spark/sql/catalyst/encoders/ExpressionEncoder;\nCaused by: NoSuchMethodError: org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply(Lorg/apache/spark/sql/types/StructType;)Lorg/apache/spark/sql/catalyst/encoders/ExpressionEncoder;",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "val volcano = spark.read.format(\"cosmos.oltp\").options(volcanoDBConfig).load()\n",
    "   volcano.createOrReplaceTempView(\"volcanotemp\")\n",
    "   volcano.show(5,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d55da5e-91b1-4d6f-99e1-d6b5a2e44126",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = spark.sql('SELECT * FROM c').toPandas()\n",
    "data = data.drop('logs', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61ff3a63-c29b-4573-85f2-df4d5670ee1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = data.groupby(['device_id','error_type','Priority', 'Last_Log'])['datetime'].max().reset_index()\n",
    "data['Status'] = 'Pending'\n",
    "data.to_csv(\"JIRA_Upload.csv\", index_label=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Log_Monitor",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
